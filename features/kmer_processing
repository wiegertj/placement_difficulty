import os
import time
from itertools import product
import pandas as pd
import statistics
import multiprocessing
from Bio import SeqIO
from probables import BloomFilter


def filter_gapped_kmers(sequence, k, max_gap_percent):
    kmers = []
    nucleotide_ambiguity_code = {
        'R': ['A', 'G'],
        'Y': ['C', 'T'],
        'S': ['G', 'C'],
        'W': ['A', 'T'],
        'K': ['G', 'T'],
        'M': ['A', 'C'],
        'B': ['C', 'G', 'T'],
        'D': ['A', 'G', 'T'],
        'H': ['A', 'C', 'T'],
        'V': ['A', 'C', 'G']
    }
    for i in range(len(sequence) - k + 1):

        kmer = sequence[i:i + k]
        gap_count = kmer.count('-')
        if gap_count / k <= max_gap_percent:

            ambiguous_positions = [i for i, char in enumerate(kmer) if char in nucleotide_ambiguity_code]

            expanded_kmers = []
            if ambiguous_positions:
                combinations = product(
                    *(nucleotide_ambiguity_code[char] for char in kmer if char in nucleotide_ambiguity_code))
                for combination in combinations:
                    expanded_kmer = list(kmer)
                    for position, nucleotide in zip(ambiguous_positions, combination):
                        expanded_kmer[position] = nucleotide
                    expanded_kmers.append(''.join(expanded_kmer))
                kmers.extend(expanded_kmers)
            else:
                kmers.append(kmer)
    return kmers


def bloom_filter(filtered_kmers):
    bf = BloomFilter(5000, 0.01)
    for kmer in filtered_kmers:
        bf.add(kmer)
    return bf


def initializer(bloom_filters_MSA_, msa_file_):
    global bloom_filters_MSA
    global msa_file
    bloom_filters_MSA = bloom_filters_MSA_
    msa_file = msa_file_


def monitor_progress(results):
    completed_count = 0
    total_count = len(results)
    start_time = time.time()

    while completed_count < total_count:
        completed_count = sum(result.ready() for result in results)

        elapsed_time = time.time() - start_time

        average_time_per_task = elapsed_time / completed_count if completed_count > 0 else 0

        remaining_count = total_count - completed_count
        remaining_time = average_time_per_task * remaining_count

        elapsed_time_str = time.strftime("%H:%M:%S", time.gmtime(elapsed_time))
        remaining_time_str = time.strftime("%H:%M:%S", time.gmtime(remaining_time))

        progress = f"Completed: {completed_count}/{total_count} tasks"
        time_estimate = f"Elapsed time: {elapsed_time_str}, Remaining time: {remaining_time_str}"
        print(progress)
        print(time_estimate)

        time.sleep(45)


def multiprocess_string_kernel(query_filename, bloom_filters_MSA, msa_file):
    data = []
    counter = 0
    for record in SeqIO.parse(os.path.join(os.pardir, "data/raw/query", query_filename), 'fasta'):
        counter += 1
        if counter > 1000 and counter <= 2000:
            data.append(record)
        if counter > 2000:
            break

    pool = multiprocessing.Pool(initializer=initializer, initargs=(bloom_filters_MSA, msa_file))
    results = [pool.apply_async(compute_string_kernel_statistics, (item,)) for item in data]
    monitor_progress(results)
    output = [result.get() for result in results]

    pool.close()
    pool.join()

    return output


def compute_string_kernel_statistics(query):
    kmers_query = filter_gapped_kmers(str(query.seq), 8, 0.4)
    query_bf = bloom_filter(kmers_query)

    result_string_kernels = []
    for bloom_filter_ref in bloom_filters_MSA:
        hash_kernel = 0
        for kmer in set(kmers_query):
            hash_kernel += bloom_filter_ref.check(kmer) * query_bf.check(kmer)

        result_string_kernels.append(hash_kernel / len(kmers_query))

    # Compute summary statistics over string kernels as features
    min_kernel = min(result_string_kernels)
    max_kernel = max(result_string_kernels)
    std_kernel = statistics.stdev(result_string_kernels)

    return (msa_file, query.id, min_kernel, max_kernel, std_kernel)


if __name__ == '__main__':

    results = []

    if multiprocessing.current_process().name == 'MainProcess':
        multiprocessing.freeze_support()

    for msa_file, query_file in [("bv_reference.fasta", "bv_query.fasta")]:

        bloom_filters_MSA = []
        string_kernel_features = []

        # Create bloom filters for each sequence in the MSA
        for record in SeqIO.parse(os.path.join(os.pardir, "data/raw/msa", msa_file), 'fasta'):
            kmers = filter_gapped_kmers(str(record.seq), 8, 0.4)
            bf = bloom_filter(kmers)
            bloom_filters_MSA.append(bf)

        result_tmp = multiprocess_string_kernel(query_file, bloom_filters_MSA, msa_file)
        results.extend(result_tmp)

    df = pd.DataFrame(results, columns=['dataset', 'sampleId', 'min_kernel', 'max_kernel', 'std_kernel'])
    df.to_csv(os.path.join(os.pardir, "data/processed/features", "bv_query_msa_query_kmer8_04_2000"))
